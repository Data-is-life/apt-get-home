{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:56:01.153743Z",
     "start_time": "2018-09-29T05:56:01.130882Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import time\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import urllib\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from selenium.webdriver.firefox.webdriver import FirefoxProfile\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import string\n",
    "from header_list import user_agent_list\n",
    "from proxies_list import proxies_list, proxie_random_pool\n",
    "from proxie_check import proxie_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:56:02.241014Z",
     "start_time": "2018-09-29T05:56:02.227024Z"
    }
   },
   "outputs": [],
   "source": [
    "ua = user_agent_list\n",
    "proxies = proxies_list\n",
    "prx_pool = proxie_random_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:56:03.325853Z",
     "start_time": "2018-09-29T05:56:03.311371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#proxie_check(proxies)\n",
    "len(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:19:18.114106Z",
     "start_time": "2018-09-29T05:19:18.099992Z"
    }
   },
   "outputs": [],
   "source": [
    "def session_creator(proxy, ua, url):\n",
    "    header = random.sample(ua, 1)[0]\n",
    "    session = requests.Session()\n",
    "    session.proxies = {\"http\": proxy, \"https\": proxy}\n",
    "    req = session.get(url, headers=header)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:19:20.306934Z",
     "start_time": "2018-09-29T05:19:20.287045Z"
    }
   },
   "outputs": [],
   "source": [
    "zip_codes = pd.read_csv('../Data/zips.csv')\n",
    "zip_list = zip_codes['zip'].tolist()\n",
    "zip_list = [num for num in zip_list if str(num) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:19:20.900884Z",
     "start_time": "2018-09-29T05:19:20.885555Z"
    }
   },
   "outputs": [],
   "source": [
    "# rand_zip_list_ap = random.sample(zip_list, len(zip_list))\n",
    "# ap_prp_count_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:19:22.242112Z",
     "start_time": "2018-09-29T05:19:22.226420Z"
    }
   },
   "outputs": [],
   "source": [
    "# rand_zip_sld = random.sample(zip_list, len(zip_list))\n",
    "# sold_prp_list = []\n",
    "# ezl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:19:23.700825Z",
     "start_time": "2018-09-29T05:19:23.685986Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zip_prop_count(zip_list, proxies, prp_list, ua, ezl):\n",
    "    proxy = random.sample(proxies, 1)[0]\n",
    "    print(proxies.index(proxy))\n",
    "    print(proxy)\n",
    "    for num in zip_list:\n",
    "        url = 'https://www.redfin.com/zipcode/' + \\\n",
    "            str(num)+'/filter/property-type=house+condo+townhouse,include=sold-1yr,min-price=100k,min-baths=1,include=sold-1yr'\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            soup = session_creator(proxy, ua, url)\n",
    "            #all_count = soup.findAll('span', {'data-rf-test-name': 'homes-for-sale'})\n",
    "            print(num)\n",
    "            print(len(zip_list))\n",
    "            all_count = soup.findAll('div', {'class': 'homes summary'})\n",
    "            if len(str(all_count)) >= 20:\n",
    "                print(all_count)\n",
    "                print(time.time() - start_time)\n",
    "                ezl.append(num)\n",
    "                prp_list.append(all_count)\n",
    "                zip_list.remove(num)\n",
    "                print(len(zip_list)+len(prp_list))\n",
    "            else:\n",
    "                print(\"Captcha!!!!!\")\n",
    "        except:\n",
    "            print(\"Skipping. Connnection error\")\n",
    "            proxies.remove(proxy)\n",
    "            print(len(proxies))\n",
    "            return prp_list, zip_list, proxies, ezl\n",
    "    return prp_list, zip_list, proxies, ezl\n",
    "\n",
    "#ap_prp_count_list, rand_zip_list, proxies = zip_prop_count(rand_zip_list_ap, proxies, ap_prp_count_list, ua)\n",
    "#sold_prp_list, rand_zip_sld, proxies, ezl = zip_prop_count(rand_zip_sld, proxies, sold_prp_list, ua, ezl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:19:29.759478Z",
     "start_time": "2018-09-29T05:19:29.736673Z"
    }
   },
   "outputs": [],
   "source": [
    "# sold_prp_list = [str(num) for num in sold_prp_list]\n",
    "# sold_prp_list = [num.replace(\n",
    "#     '[<div class=\"homes summary\" data-rf-test-id=\"homes-description\"><span class=\"showingText\">Showing </span>', '') for num in sold_prp_list]\n",
    "# sold_prp_list = [num.replace(\n",
    "#     ' Homes<span class=\"summarySeparator \">â€¢</span></div>]', '') for num in sold_prp_list]\n",
    "# sold_prp_list = [num.replace('20 of ', '') for num in sold_prp_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:19:32.373973Z",
     "start_time": "2018-09-29T05:19:32.354703Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_count(lst):\n",
    "    rx_num_homes = r'\\d+\\shomes'\n",
    "    rx_zip = r'\\d+\\sat'\n",
    "    rx_median = r'\\$\\d+\\.?\\w+'\n",
    "    median_list = []\n",
    "    zip_list = []\n",
    "    num_homes_list = []\n",
    "    for num in lst:\n",
    "        num = str(num)\n",
    "        median_ = re.findall(rx_median, num, re.MULTILINE)\n",
    "        zip_ = re.findall(rx_zip, num, re.MULTILINE)\n",
    "        num_homes_ = re.findall(rx_num_homes, num, re.MULTILINE)\n",
    "        median_list.extend([i for i in median_])\n",
    "        zip_list.extend([i for i in zip_])\n",
    "        num_homes_list.extend([i for i in num_homes_])\n",
    "\n",
    "    median_list = [num.replace('$', '').replace(\n",
    "        'K', ',000').replace('.', ',') for num in median_list]\n",
    "    i = 0\n",
    "    while i < len(median_list):\n",
    "        if len(median_list[i]) == 2:\n",
    "            median_list[i] = median_list[i].replace('M', ',000,000')\n",
    "            i += 1\n",
    "        elif len(median_list[i]) == 4:\n",
    "            median_list[i] = median_list[i].replace('M', '00,000')\n",
    "            i += 1\n",
    "        elif len(median_list[i]) == 5:\n",
    "            median_list[i] = median_list[i].replace('M', '0,000')\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    zip_list = [re.findall(r'\\d+', num, re.MULTILINE)[0] for num in zip_list]\n",
    "    num_homes_list = [re.findall(r'\\d+', num, re.MULTILINE)[0]\n",
    "                      for num in num_homes_list]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data={'zip': zip_list, 'median_price': median_list, 'num_ap_homes': num_homes_list})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:19:36.263815Z",
     "start_time": "2018-09-29T05:19:36.248511Z"
    }
   },
   "outputs": [],
   "source": [
    "# sdf = pd.DataFrame({'zip': ezl, 'sld': sold_prp_list})\n",
    "# apdf = strip_count(ap_prp_count_list)\n",
    "# apdf.to_csv('../Data/apdf.csv')\n",
    "# sdf.to_csv('../Data/sdf.csv')\n",
    "\n",
    "# apdf.index = ap_df['zip']\n",
    "# sdf.index = sdf['zip']\n",
    "\n",
    "# sdf['sld'] = (sdf['sld']).astype(int)\n",
    "# apdf['num_ap_homes'] = (ap_df['num_ap_homes']).astype(int)\n",
    "\n",
    "# sdf['zip'] = (sdf['zip']).astype(int)\n",
    "# apdf['zip'] = (ap_df['zip']).astype(int)\n",
    "\n",
    "# sld_pgs = [ceil(num/20) for num in sdf['sld']]\n",
    "# ap_pgs = [ceil(num/20) for num in apdf['num_ap_homes']]\n",
    "\n",
    "# sdf['pages'] = sld_pgs\n",
    "# apdf['pages'] = ap_pgs\n",
    "\n",
    "# sdf = sdf.sort_values(by='pages')\n",
    "# apdf = apdf.sort_values(by='pages')\n",
    "\n",
    "# sdf_under = sdf[sdf['pages'] <= 18]\n",
    "# sdf_over = sdf[sdf['pages'] > 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:55:14.489310Z",
     "start_time": "2018-09-29T05:55:14.470005Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-0371d44a0573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0map_url_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnum_pages_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zip'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pages_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.redfin.com/zipcode/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m             \u001b[0;34m'/filter/property-type=house+condo+townhouse,min-baths=1,status=active+pending+contingent/page-'\u001b[0m \u001b[0;34m+\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'apdf' is not defined"
     ]
    }
   ],
   "source": [
    "ap_url_list = []\n",
    "for num in zip_list:\n",
    "    num_pages_count = apdf.loc[apdf['zip'] == num].pages.values[0]\n",
    "    for i in range(1, num_pages_count+1):\n",
    "        url = 'https://www.redfin.com/zipcode/'+str(num) + \\\n",
    "            '/filter/property-type=house+condo+townhouse,min-baths=1,status=active+pending+contingent/page-' + \\\n",
    "            str(i)\n",
    "        ap_url_list.append(url)\n",
    "\n",
    "\n",
    "ap_url_list = [num.replace(r'/page-1^', '') for num in ap_url_list]\n",
    "# ap_url_list = [num.replace('contingent0', 'contingent/page-10')\n",
    "#                for num in ap_url_list]\n",
    "# ap_url_list = [num.replace('contingent1', 'contingent/page-11')\n",
    "#                for num in ap_url_list]\n",
    "# ap_url_list = [num.replace('contingent2', 'contingent/page-12')\n",
    "#                for num in ap_url_list]\n",
    "# ap_url_list = [num.replace('contingent3', 'contingent/page-13')\n",
    "#                for num in ap_url_list]\n",
    "\n",
    "# main_df = pd.DataFrame(columns=['full_address', 'home_link'])\n",
    "\n",
    "ap_url_list = sorted(ap_url_list)\n",
    "print(ap_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:43:55.188445Z",
     "start_time": "2018-09-28T20:43:55.172135Z"
    }
   },
   "outputs": [],
   "source": [
    "# sld_under_url_list = []\n",
    "\n",
    "# for num in sdf_under['zip']:\n",
    "#     num_pages_count = sdf_under.loc[sdf_under['zip'] == num].pages.values[0]\n",
    "#     for i in range(1, num_pages_count+1):\n",
    "#         url = 'https://www.redfin.com/zipcode/'+str(num) + \\\n",
    "#             '/filter/property-type=house+condo+townhouse,min-price=100k,min-baths=1,include=sold-1yr/page-' + \\\n",
    "#             str(i)\n",
    "#         sld_under_url_list.append(url)\n",
    "\n",
    "\n",
    "# sld_under_url_list = [num.replace('/page-1', '') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr0', '=sold-1yr/page-10') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr1', '=sold-1yr/page-11') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr2', '=sold-1yr/page-12') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr3', '=sold-1yr/page-13') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr4', '=sold-1yr/page-14') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr5', '=sold-1yr/page-15') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr6', '=sold-1yr/page-16') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr7', '=sold-1yr/page-17') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr8', '=sold-1yr/page-18') for num in sld_under_url_list]\n",
    "\n",
    "# s_main_df = pd.DataFrame(columns=['full_address', 'home_link'])\n",
    "\n",
    "sld_under_url_list = sorted(sld_under_url_list)\n",
    "# print(len(sld_under_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:04.073501Z",
     "start_time": "2018-09-28T20:44:04.057370Z"
    }
   },
   "outputs": [],
   "source": [
    "def each_page(proxy, ua, url):\n",
    "    soup = session_creator(proxy, ua, url)\n",
    "    start_time = time.time()\n",
    "    time.sleep(random.uniform(0, 1)*4)\n",
    "    print(time.time()-start_time)\n",
    "    full_soup = soup.findAll('a', {'class': 'bottom link-override'})\n",
    "    full_address = [fas['title'] for fas in full_soup]\n",
    "    home_link = ['https://www.redfin.com' +\n",
    "                 str(hls.get('href')) for hls in full_soup]\n",
    "    df = {'full_address': full_address, 'home_link': home_link}\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:20:01.738266Z",
     "start_time": "2018-09-29T05:20:01.723708Z"
    }
   },
   "outputs": [],
   "source": [
    "def links_for_props(proxies, url_list, main_df, ua):\n",
    "    proxy = random.sample(proxies, 1)[0]\n",
    "    print(proxies.index(proxy))\n",
    "    print(proxy)\n",
    "    i = 0\n",
    "    while i < len(url_list):\n",
    "        url = url_list[i]        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            print(len(url_list))\n",
    "            print(url)\n",
    "            \n",
    "            data = each_page(proxy, ua, url)\n",
    "            df = pd.DataFrame(data)\n",
    "            eds = {'full_address': [], 'home_link': []}\n",
    "            if data['full_address'] != eds['full_address']:\n",
    "                main_df = pd.concat([main_df, df])\n",
    "                url_list.pop(i)\n",
    "                print(time.time() - start_time)\n",
    "            else:\n",
    "                print('Captcha')\n",
    "                return url_list, main_df\n",
    "        except:\n",
    "            print(\"Skipping. Connnection error\")\n",
    "            proxies.remove(proxy)\n",
    "            print(len(proxies))\n",
    "\n",
    "            return url_list, main_df\n",
    "\n",
    "    return url_list, main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:05.885808Z",
     "start_time": "2018-09-28T20:44:05.868773Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a = randint(0,3)\n",
    "# time.sleep(random.uniform(0,1)*((88/13)**.33)*a)\n",
    "# ap_url_list, main_df = links_for_props(proxies,ap_url_list, main_df, ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:06.749963Z",
     "start_time": "2018-09-28T20:44:06.733486Z"
    }
   },
   "outputs": [],
   "source": [
    "# main_df.to_csv('../Data/main_df_ap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:07.669381Z",
     "start_time": "2018-09-28T20:44:07.649615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# sld_under_url_list = sld_under_url_list[1:]\n",
    "print(len(sld_under_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:08.886948Z",
     "start_time": "2018-09-28T20:44:08.872336Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sld_under_url_list, s_main_df = links_for_props(\n",
    "#     proxies, sld_under_url_list, s_main_df, ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:16.925278Z",
     "start_time": "2018-09-28T20:44:16.908689Z"
    }
   },
   "outputs": [],
   "source": [
    "list_ = list(s_main_df.full_address.values)\n",
    "#set([num[-5:] for num in list_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:24:54.736939Z",
     "start_time": "2018-09-28T23:24:54.658430Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_main_df = pd.read_csv('../Data/s_main.csv')\n",
    "main_df = pd.read_csv('../Data/main_df_ap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T09:32:50.228058Z",
     "start_time": "2018-09-28T09:32:50.145856Z"
    }
   },
   "outputs": [],
   "source": [
    "# s_main_df.to_csv('../Data/s_main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:25:43.229063Z",
     "start_time": "2018-09-28T23:25:43.171054Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     all_home_feats = soup.findAll('div', {'class':\"amenities-container\"})\n",
    "#     for num in all_home_feats:\n",
    "#         print(num.findAll('li'))\n",
    "\n",
    "#     other_home_feats = soup.findAll('span', {'data-rf-test-id':\"propertyDetails\"})\n",
    "#     print(f'other_home_feats = {other_home_feats}')\n",
    "#     other_home_feats_vals = soup.findAll('span', {'class':\"content font-weight-roman\"})\n",
    "#     print(f'other_home_feats_vals = {other_home_feats_vals}')\n",
    "    #both are in soup.findAll('div', {'class':\"keyDetail font-size-base\"})\n",
    "\n",
    "#     home_full_feat_desc = soup.findAll('span', {'class': 'statsLabel'})\n",
    "#     for desc in home_full_feat_desc:\n",
    "#         print(f'hffd = {desc}')\n",
    "#         homefeat_desc.append(str(desc.contents[0]))\n",
    "\n",
    "#     home_full_feat_info = soup.findAll('span', {'class': 'statsValue'})\n",
    "#     for info in home_full_feat_info:\n",
    "#         print(f'info = {info}')\n",
    "#         homefeat_info.append(str(info.contents[0]))\n",
    "\n",
    "#     school_grades = soup.findAll('div', {'class': 'nearby-schools-grades'})\n",
    "#     for grade in school_grades:\n",
    "#         print(grade.text)\n",
    "\n",
    "#     school_assigned = soup.findAll(\n",
    "#         'span', {'class': 'assigned-label zsg-fineprint'})\n",
    "\n",
    "#     for assi in school_assigned:\n",
    "#         print(assi.text)\n",
    "\n",
    "#     school_dist = soup.findAll('div', {'class': 'nearby-schools-distance'})\n",
    "#     for dist in school_dist:\n",
    "#         print(dist.text)\n",
    "\n",
    "#     school_rating = soup.findAll('div', {'class': 'nearby-schools-rating'})\n",
    "#     for rating in school_rating:\n",
    "#         print(rating.span.text)\n",
    "\n",
    "#     school_name_regex = r'school-name notranslate$'\n",
    "#     school_name = soup.findAll('a', {'class': 'school-name notranslate'})\n",
    "#     for mane in school_name:\n",
    "#         print(mane.next_element)\n",
    "\n",
    "#     homefeat_desc = []\n",
    "#     homefeat_info = []\n",
    "\n",
    "#     new_dict = dict(zip(homefeat_desc, homefeat_info))\n",
    "#     new_dict['address'] = str(home_address_MLS)\n",
    "\n",
    "# #     new_dict['home_description'] = str(home_description.attrs['content'])\n",
    "\n",
    "# lots of info:\n",
    "\n",
    "#     estimate = soup.find('div', {'data-rf-test-name': 'avmValue\"'})\n",
    "#     print(f'estimate = {estimate}')\n",
    "\n",
    "#     home_hist_src = soup.findAll('td', {'class': 'zsg-sm-hide'})\n",
    "#     print(home_hist_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:51:33.586458Z",
     "start_time": "2018-09-29T05:51:33.569254Z"
    }
   },
   "outputs": [],
   "source": [
    "def each_property(url, hdr, proxy):\n",
    "    soup = session_creator(proxy, ua, url)\n",
    "\n",
    "    price_regex = r'\\$\\S+\\s+'\n",
    "    bed_regex = r'\\d+\\S?\\d?\\d?Bed'\n",
    "    bath_regex = r'\\d+\\S?\\d?\\d?Bath'\n",
    "    size_regex = r'\\d+\\S?\\d?\\d?\\s?Sq'\n",
    "    yr_blt_regex = r'Built: \\d+'\n",
    "    status_regex = r'Status: \\w+'\n",
    "\n",
    "    home_address_MLS = soup.title.string\n",
    "\n",
    "    all_top = soup.findAll('div', {'class': 'HomeInfo inline-block'})\n",
    "    all_top_list = []\n",
    "    for num in all_top:\n",
    "        a = num.text\n",
    "        all_top_list.extend(price_regex, a)\n",
    "        all_top_list.extend(bed_regex, a)\n",
    "        all_top_list.extend(bath_regex, a)\n",
    "        all_top_list.extend(size_regex, a)\n",
    "        all_top_list.extend(yr_blt_regex, a)\n",
    "        all_top_list.extend(status_regex, a)\n",
    "\n",
    "    home_description = soup.find('p', {'class': 'font-b1'})\n",
    "    home_desc_text = home_description.span.text\n",
    "\n",
    "    all_home_feats = soup.findAll('span', {'class': \"entryItemContent\"})\n",
    "    feats = [num.text for num in all_home_feats]\n",
    "    \n",
    "    prop_hist = soup.findAll('div', {'id': 'propertyHistory-expandable-segment'})\n",
    "    prop_history = [num.text for num in prop_hist]\n",
    "\n",
    "    school_info = soup.findAll('div', {'class': \"name-and-info\"})\n",
    "    schools = [num.text for num in school_info]\n",
    "    \n",
    "    return home_address_MLS, all_top_list, home_desc_text, feats, prop_history, schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:53:49.247959Z",
     "start_time": "2018-09-29T05:52:53.117359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.123.99.70:38913\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "extend() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-f9475af8f434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhome_address_MLS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_top_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhome_desc_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-18a4bba35f4a>\u001b[0m in \u001b[0;36meach_property\u001b[0;34m(url, hdr, proxy)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_top\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mall_top_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice_regex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mall_top_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbed_regex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mall_top_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbath_regex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: extend() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "header = random.sample(ua, 1)[0]\n",
    "proxy = random.sample(proxies, 1)[0]\n",
    "sold_url_list = s_main_df['home_link'].tolist()\n",
    "url = random.sample(sold_url_list, 1)[0]\n",
    "print(proxy)\n",
    "\n",
    "home_address_MLS, all_top_list, home_desc_text, feats, prop_history, schools = each_property(url, header, proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:46:20.669881Z",
     "start_time": "2018-09-29T05:46:20.657381Z"
    }
   },
   "outputs": [],
   "source": [
    "price_regex = r'\\$\\S+\\s+'\n",
    "bed_regex = r'\\d+\\S?\\d?\\d?Bed'\n",
    "bath_regex = r'\\d+\\S?\\d?\\d?Bath'\n",
    "size_regex = r'\\d+\\S?\\d?\\d?\\s?Sq'\n",
    "yr_blt_regex = r'Built: \\d+'\n",
    "status_regex = r'Status: \\w+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T05:46:20.691895Z",
     "start_time": "2018-09-29T05:46:20.671582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ptp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
