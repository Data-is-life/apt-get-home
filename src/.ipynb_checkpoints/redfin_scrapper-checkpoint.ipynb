{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:03:46.177867Z",
     "start_time": "2018-09-28T23:03:45.299798Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import time\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import urllib\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from selenium.webdriver.firefox.webdriver import FirefoxProfile\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T04:46:27.919799Z",
     "start_time": "2018-09-29T04:46:27.899175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from header_list import user_agent_list\n",
    "from proxies_list import proxies_list, proxie_random_pool\n",
    "ua = user_agent_list\n",
    "proxies = proxies_list\n",
    "prx_pool = proxie_random_pool\n",
    "len(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:23:32.053313Z",
     "start_time": "2018-09-28T23:23:32.034518Z"
    }
   },
   "outputs": [],
   "source": [
    "def proxie_check(proxies):\n",
    "    default_list = []\n",
    "    url = 'https://httpbin.org/ip'\n",
    "    for i in range(0, len(proxies)):\n",
    "        proxy = proxies[i]\n",
    "        print(i+1)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url, proxies={\"http\": proxy, \"https\": proxy})\n",
    "            print(response.json())\n",
    "            print(time.time() - start_time)\n",
    "        except:\n",
    "            print(\"Skipping. Connnection error\")\n",
    "            default_list.append(i+1)\n",
    "            print(time.time() - start_time)\n",
    "        print(default_list)\n",
    "    return default_list\n",
    "proxie_check(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:09:43.296139Z",
     "start_time": "2018-09-28T23:09:43.280554Z"
    }
   },
   "outputs": [],
   "source": [
    "def session_creator(proxy, ua, url):\n",
    "    header = random.sample(ua, 1)[0]\n",
    "    session = requests.Session()\n",
    "    session.proxies = {\"http\": proxy, \"https\": proxy}\n",
    "    req = session.get(url, headers=header)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:23:34.532281Z",
     "start_time": "2018-09-28T23:23:34.514701Z"
    }
   },
   "outputs": [],
   "source": [
    "def no_proxy_session(ua, url):\n",
    "    header = random.sample(ua, 1)[0]\n",
    "    session = requests.Session()\n",
    "    req = session.get(url, headers=header)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:23:37.014139Z",
     "start_time": "2018-09-28T23:23:36.917284Z"
    }
   },
   "outputs": [],
   "source": [
    "zip_codes = pd.read_csv('../Data/zips.csv')\n",
    "zip_list = zip_codes['zip'].tolist()\n",
    "zip_list = [num for num in zip_list if str(num) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:23:37.457938Z",
     "start_time": "2018-09-28T23:23:37.441433Z"
    }
   },
   "outputs": [],
   "source": [
    "# rand_zip_list_ap = random.sample(zip_list, len(zip_list))\n",
    "# ap_prp_count_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:23:38.052383Z",
     "start_time": "2018-09-28T23:23:38.036609Z"
    }
   },
   "outputs": [],
   "source": [
    "# rand_zip_sld = random.sample(zip_list, len(zip_list))\n",
    "# sold_prp_list = []\n",
    "# ezl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:23:38.776794Z",
     "start_time": "2018-09-28T23:23:38.760066Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zip_prop_count(zip_list, proxies, prp_list, ua, ezl):\n",
    "    proxy = random.sample(proxies, 1)[0]\n",
    "    print(proxies.index(proxy))\n",
    "    print(proxy)\n",
    "    for num in zip_list:\n",
    "        url = 'https://www.redfin.com/zipcode/' + \\\n",
    "            str(num)+'/filter/property-type=house+condo+townhouse,include=sold-1yr,min-price=100k,min-baths=1,include=sold-1yr'\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            soup = session_creator(proxy, ua, url)\n",
    "            #all_count = soup.findAll('span', {'data-rf-test-name': 'homes-for-sale'})\n",
    "            print(num)\n",
    "            print(len(zip_list))\n",
    "            all_count = soup.findAll('div', {'class': 'homes summary'})\n",
    "            if len(str(all_count)) >= 20:\n",
    "                print(all_count)\n",
    "                print(time.time() - start_time)\n",
    "                ezl.append(num)\n",
    "                prp_list.append(all_count)\n",
    "                zip_list.remove(num)\n",
    "                print(len(zip_list)+len(prp_list))\n",
    "            else:\n",
    "                print(\"Captcha!!!!!\")\n",
    "        except:\n",
    "            print(\"Skipping. Connnection error\")\n",
    "            proxies.remove(proxy)\n",
    "            print(len(proxies))\n",
    "            return prp_list, zip_list, proxies, ezl\n",
    "    return prp_list, zip_list, proxies, ezl\n",
    "\n",
    "#ap_prp_count_list, rand_zip_list, proxies = zip_prop_count(rand_zip_list_ap, proxies, ap_prp_count_list, ua)\n",
    "#sold_prp_list, rand_zip_sld, proxies, ezl = zip_prop_count(rand_zip_sld, proxies, sold_prp_list, ua, ezl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:23:40.916340Z",
     "start_time": "2018-09-28T23:23:40.807890Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sold_prp_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e69a4f3b1757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msold_prp_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msold_prp_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m sold_prp_list = [num.replace(\n\u001b[1;32m      3\u001b[0m     '[<div class=\"homes summary\" data-rf-test-id=\"homes-description\"><span class=\"showingText\">Showing </span>', '') for num in sold_prp_list]\n\u001b[1;32m      4\u001b[0m sold_prp_list = [num.replace(\n\u001b[1;32m      5\u001b[0m     ' Homes<span class=\"summarySeparator \">•</span></div>]', '') for num in sold_prp_list]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sold_prp_list' is not defined"
     ]
    }
   ],
   "source": [
    "sold_prp_list = [str(num) for num in sold_prp_list]\n",
    "sold_prp_list = [num.replace(\n",
    "    '[<div class=\"homes summary\" data-rf-test-id=\"homes-description\"><span class=\"showingText\">Showing </span>', '') for num in sold_prp_list]\n",
    "sold_prp_list = [num.replace(\n",
    "    ' Homes<span class=\"summarySeparator \">•</span></div>]', '') for num in sold_prp_list]\n",
    "sold_prp_list = [num.replace('20 of ', '') for num in sold_prp_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:23:48.624682Z",
     "start_time": "2018-09-28T23:23:48.605640Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_count(lst):\n",
    "    rx_num_homes = r'\\d+\\shomes'\n",
    "    rx_zip = r'\\d+\\sat'\n",
    "    rx_median = r'\\$\\d+\\.?\\w+'\n",
    "    median_list = []\n",
    "    zip_list = []\n",
    "    num_homes_list = []\n",
    "    for num in lst:\n",
    "        num = str(num)\n",
    "        median_ = re.findall(rx_median, num, re.MULTILINE)\n",
    "        zip_ = re.findall(rx_zip, num, re.MULTILINE)\n",
    "        num_homes_ = re.findall(rx_num_homes, num, re.MULTILINE)\n",
    "        median_list.extend([i for i in median_])\n",
    "        zip_list.extend([i for i in zip_])\n",
    "        num_homes_list.extend([i for i in num_homes_])\n",
    "\n",
    "    median_list = [num.replace('$', '').replace(\n",
    "        'K', ',000').replace('.', ',') for num in median_list]\n",
    "    i = 0\n",
    "    while i < len(median_list):\n",
    "        if len(median_list[i]) == 2:\n",
    "            median_list[i] = median_list[i].replace('M', ',000,000')\n",
    "            i += 1\n",
    "        elif len(median_list[i]) == 4:\n",
    "            median_list[i] = median_list[i].replace('M', '00,000')\n",
    "            i += 1\n",
    "        elif len(median_list[i]) == 5:\n",
    "            median_list[i] = median_list[i].replace('M', '0,000')\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    zip_list = [re.findall(r'\\d+', num, re.MULTILINE)[0] for num in zip_list]\n",
    "    num_homes_list = [re.findall(r'\\d+', num, re.MULTILINE)[0]\n",
    "                      for num in num_homes_list]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data={'zip': zip_list, 'median_price': median_list, 'num_ap_homes': num_homes_list})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T04:19:43.027811Z",
     "start_time": "2018-09-28T04:19:42.996058Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = pd.DataFrame({'zip': ezl, 'sld': sold_prp_list})\n",
    "apdf = strip_count(ap_prp_count_list)\n",
    "apdf.to_csv('../Data/apdf.csv')\n",
    "sdf.to_csv('../Data/sdf.csv')\n",
    "\n",
    "apdf.index = ap_df['zip']\n",
    "sdf.index = sdf['zip']\n",
    "\n",
    "sdf['sld'] = (sdf['sld']).astype(int)\n",
    "apdf['num_ap_homes'] = (ap_df['num_ap_homes']).astype(int)\n",
    "\n",
    "sdf['zip'] = (sdf['zip']).astype(int)\n",
    "apdf['zip'] = (ap_df['zip']).astype(int)\n",
    "\n",
    "sld_pgs = [ceil(num/20) for num in sdf['sld']]\n",
    "ap_pgs = [ceil(num/20) for num in apdf['num_ap_homes']]\n",
    "\n",
    "sdf['pages'] = sld_pgs\n",
    "apdf['pages'] = ap_pgs\n",
    "\n",
    "sdf = sdf.sort_values(by='pages')\n",
    "apdf = apdf.sort_values(by='pages')\n",
    "\n",
    "sdf_under = sdf[sdf['pages'] <= 18]\n",
    "sdf_over = sdf[sdf['pages'] > 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T09:34:27.766704Z",
     "start_time": "2018-09-28T09:34:27.684322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446\n"
     ]
    }
   ],
   "source": [
    "ap_url_list = []\n",
    "for num in zip_list:\n",
    "    num_pages_count = apdf.loc[apdf['zip'] == num].pages.values[0]\n",
    "    for i in range(1, num_pages_count+1):\n",
    "        url = 'https://www.redfin.com/zipcode/'+str(num) + \\\n",
    "            '/filter/property-type=house+condo+townhouse,min-baths=1,status=active+pending+contingent/page-' + \\\n",
    "            str(i)\n",
    "        ap_url_list.append(url)\n",
    "\n",
    "\n",
    "ap_url_list = [num.replace('/page-1', '') for num in ap_url_list]\n",
    "ap_url_list = [num.replace('contingent0', 'contingent/page-10')\n",
    "               for num in ap_url_list]\n",
    "ap_url_list = [num.replace('contingent1', 'contingent/page-11')\n",
    "               for num in ap_url_list]\n",
    "ap_url_list = [num.replace('contingent2', 'contingent/page-12')\n",
    "               for num in ap_url_list]\n",
    "ap_url_list = [num.replace('contingent3', 'contingent/page-13')\n",
    "               for num in ap_url_list]\n",
    "\n",
    "# main_df = pd.DataFrame(columns=['full_address', 'home_link'])\n",
    "\n",
    "ap_url_list = sorted(ap_url_list)\n",
    "print(len(ap_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:43:55.188445Z",
     "start_time": "2018-09-28T20:43:55.172135Z"
    }
   },
   "outputs": [],
   "source": [
    "# sld_under_url_list = []\n",
    "\n",
    "# for num in sdf_under['zip']:\n",
    "#     num_pages_count = sdf_under.loc[sdf_under['zip'] == num].pages.values[0]\n",
    "#     for i in range(1, num_pages_count+1):\n",
    "#         url = 'https://www.redfin.com/zipcode/'+str(num) + \\\n",
    "#             '/filter/property-type=house+condo+townhouse,min-price=100k,min-baths=1,include=sold-1yr/page-' + \\\n",
    "#             str(i)\n",
    "#         sld_under_url_list.append(url)\n",
    "\n",
    "\n",
    "# sld_under_url_list = [num.replace('/page-1', '') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr0', '=sold-1yr/page-10') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr1', '=sold-1yr/page-11') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr2', '=sold-1yr/page-12') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr3', '=sold-1yr/page-13') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr4', '=sold-1yr/page-14') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr5', '=sold-1yr/page-15') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr6', '=sold-1yr/page-16') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr7', '=sold-1yr/page-17') for num in sld_under_url_list]\n",
    "# sld_under_url_list = [num.replace(\n",
    "#     '=sold-1yr8', '=sold-1yr/page-18') for num in sld_under_url_list]\n",
    "\n",
    "# s_main_df = pd.DataFrame(columns=['full_address', 'home_link'])\n",
    "\n",
    "sld_under_url_list = sorted(sld_under_url_list)\n",
    "# print(len(sld_under_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:04.073501Z",
     "start_time": "2018-09-28T20:44:04.057370Z"
    }
   },
   "outputs": [],
   "source": [
    "def each_page(proxy, ua, url):\n",
    "    soup = session_creator(proxy, ua, url)\n",
    "    start_time = time.time()\n",
    "    time.sleep(random.uniform(0, 1)*4)\n",
    "    print(time.time()-start_time)\n",
    "    full_soup = soup.findAll('a', {'class': 'bottom link-override'})\n",
    "    full_address = [fas['title'] for fas in full_soup]\n",
    "    home_link = ['https://www.redfin.com' +\n",
    "                 str(hls.get('href')) for hls in full_soup]\n",
    "    df = {'full_address': full_address, 'home_link': home_link}\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:04.911820Z",
     "start_time": "2018-09-28T20:44:04.887342Z"
    }
   },
   "outputs": [],
   "source": [
    "def links_for_props(proxies, url_list, main_df, ua):\n",
    "    proxy = random.sample(proxies, 1)[0]\n",
    "    print(proxies.index(proxy))\n",
    "    print(proxy)\n",
    "    i = 0\n",
    "    while i < len(url_list):\n",
    "        url = url_list[i]        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            print(len(url_list))\n",
    "            print(url)\n",
    "            \n",
    "            data = each_page(proxy, ua, url)\n",
    "            df = pd.DataFrame(data)\n",
    "            eds = {'full_address': [], 'home_link': []}\n",
    "            if data['full_address'] != eds['full_address']:\n",
    "                main_df = pd.concat([main_df, df])\n",
    "                url_list.pop(i)\n",
    "                print(time.time() - start_time)\n",
    "            else:\n",
    "                print('Captcha')\n",
    "                return url_list, main_df\n",
    "        except:\n",
    "            print(\"Skipping. Connnection error\")\n",
    "            proxies.remove(proxy)\n",
    "            print(len(proxies))\n",
    "\n",
    "            return url_list, main_df\n",
    "\n",
    "    return url_list, main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:05.885808Z",
     "start_time": "2018-09-28T20:44:05.868773Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a = randint(0,3)\n",
    "# time.sleep(random.uniform(0,1)*((88/13)**.33)*a)\n",
    "# ap_url_list, main_df = links_for_props(proxies,ap_url_list, main_df, ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:06.749963Z",
     "start_time": "2018-09-28T20:44:06.733486Z"
    }
   },
   "outputs": [],
   "source": [
    "# main_df.to_csv('../Data/main_df_ap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:07.669381Z",
     "start_time": "2018-09-28T20:44:07.649615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# sld_under_url_list = sld_under_url_list[1:]\n",
    "print(len(sld_under_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:08.886948Z",
     "start_time": "2018-09-28T20:44:08.872336Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sld_under_url_list, s_main_df = links_for_props(\n",
    "#     proxies, sld_under_url_list, s_main_df, ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:44:16.925278Z",
     "start_time": "2018-09-28T20:44:16.908689Z"
    }
   },
   "outputs": [],
   "source": [
    "list_ = list(s_main_df.full_address.values)\n",
    "#set([num[-5:] for num in list_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:24:54.736939Z",
     "start_time": "2018-09-28T23:24:54.658430Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_main_df = pd.read_csv('../Data/s_main.csv')\n",
    "main_df = pd.read_csv('../Data/main_df_ap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T09:32:50.228058Z",
     "start_time": "2018-09-28T09:32:50.145856Z"
    }
   },
   "outputs": [],
   "source": [
    "# s_main_df.to_csv('../Data/s_main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T23:25:43.229063Z",
     "start_time": "2018-09-28T23:25:43.171054Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     all_home_feats = soup.findAll('div', {'class':\"amenities-container\"})\n",
    "#     for num in all_home_feats:\n",
    "#         print(num.findAll('li'))\n",
    "\n",
    "#     other_home_feats = soup.findAll('span', {'data-rf-test-id':\"propertyDetails\"})\n",
    "#     print(f'other_home_feats = {other_home_feats}')\n",
    "#     other_home_feats_vals = soup.findAll('span', {'class':\"content font-weight-roman\"})\n",
    "#     print(f'other_home_feats_vals = {other_home_feats_vals}')\n",
    "    #both are in soup.findAll('div', {'class':\"keyDetail font-size-base\"})\n",
    "\n",
    "#     home_full_feat_desc = soup.findAll('span', {'class': 'statsLabel'})\n",
    "#     for desc in home_full_feat_desc:\n",
    "#         print(f'hffd = {desc}')\n",
    "#         homefeat_desc.append(str(desc.contents[0]))\n",
    "\n",
    "#     home_full_feat_info = soup.findAll('span', {'class': 'statsValue'})\n",
    "#     for info in home_full_feat_info:\n",
    "#         print(f'info = {info}')\n",
    "#         homefeat_info.append(str(info.contents[0]))\n",
    "\n",
    "#     school_grades = soup.findAll('div', {'class': 'nearby-schools-grades'})\n",
    "#     for grade in school_grades:\n",
    "#         print(grade.text)\n",
    "\n",
    "#     school_assigned = soup.findAll(\n",
    "#         'span', {'class': 'assigned-label zsg-fineprint'})\n",
    "\n",
    "#     for assi in school_assigned:\n",
    "#         print(assi.text)\n",
    "\n",
    "#     school_dist = soup.findAll('div', {'class': 'nearby-schools-distance'})\n",
    "#     for dist in school_dist:\n",
    "#         print(dist.text)\n",
    "\n",
    "#     school_rating = soup.findAll('div', {'class': 'nearby-schools-rating'})\n",
    "#     for rating in school_rating:\n",
    "#         print(rating.span.text)\n",
    "\n",
    "#     school_name_regex = r'school-name notranslate$'\n",
    "#     school_name = soup.findAll('a', {'class': 'school-name notranslate'})\n",
    "#     for mane in school_name:\n",
    "#         print(mane.next_element)\n",
    "\n",
    "#     homefeat_desc = []\n",
    "#     homefeat_info = []\n",
    "\n",
    "#     new_dict = dict(zip(homefeat_desc, homefeat_info))\n",
    "#     new_dict['address'] = str(home_address_MLS)\n",
    "\n",
    "# #     new_dict['home_description'] = str(home_description.attrs['content'])\n",
    "\n",
    "# lots of info:\n",
    "\n",
    "#     estimate = soup.find('div', {'data-rf-test-name': 'avmValue\"'})\n",
    "#     print(f'estimate = {estimate}')\n",
    "\n",
    "#     home_hist_src = soup.findAll('td', {'class': 'zsg-sm-hide'})\n",
    "#     print(home_hist_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T02:19:54.889471Z",
     "start_time": "2018-09-29T02:19:54.873981Z"
    }
   },
   "outputs": [],
   "source": [
    "def each_property(url, hdr, proxy):\n",
    "    soup = session_creator(proxy, ua, url)\n",
    "\n",
    "    price_regex = r'\\$\\S+\\s+'\n",
    "    bed_regex = r'\\d+\\S?\\d?\\d?Bed'\n",
    "    bath_regex = r'\\d+\\S?\\d?\\d?Bath'\n",
    "    size_regex = r'\\d+\\S?\\d?\\d?\\s?Sq'\n",
    "    yr_blt_regex = r'Built: \\d+'\n",
    "    status_regex = r'Status: \\w+'\n",
    "\n",
    "    home_address_MLS = soup.title.string\n",
    "    print(home_address_MLS)\n",
    "\n",
    "    all_top = soup.findAll('div', {'class': 'HomeInfo inline-block'})\n",
    "    for num in all_top:\n",
    "        a = num.text\n",
    "        print(re.findall(price_regex, a))\n",
    "        print(re.findall(bed_regex, a))\n",
    "        print(re.findall(bath_regex, a))\n",
    "        print(re.findall(size_regex, a))\n",
    "        print(re.findall(yr_blt_regex, a))\n",
    "        print(re.findall(status_regex, a))\n",
    "\n",
    "    home_description = soup.find('p', {'class': 'font-b1'})\n",
    "    print(f'home_desc = {home_description.span}')\n",
    "\n",
    "    all_home_feats = soup.findAll('span', {'class': \"entryItemContent\"})\n",
    "    feats = [num.text for num in all_home_feats]\n",
    "    print(feats)\n",
    "    \n",
    "    prop_hist = soup.findAll('div', {'id': 'propertyHistory-expandable-segment'})\n",
    "    prop_history = [num.text for num in prop_hist]\n",
    "    print(prop_history)\n",
    "\n",
    "    school_info = soup.findAll('div', {'class': \"name-and-info\"})\n",
    "    schools = [num.text for num in school_info]\n",
    "    print(schools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T02:19:58.267019Z",
     "start_time": "2018-09-29T02:19:56.342823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.79.3.19:50940\n",
      "636 Beacon St #2, Oakland, CA 94610 | MLS# 40787243 | Redfin\n",
      "['$552,452Redfin ', '$390,000Last ', '$485 ']\n",
      "['1Bed']\n",
      "['1Bath']\n",
      "['138 Sq']\n",
      "['Built: 1922']\n",
      "['Status: Sold']\n",
      "home_desc = <span>Fantastic Opportunity for Investment!  Unit Amenities:  Naturally, sunny &amp; bright upgraded TIC unit featuring 1 spacious bedroom, 1 bathroom, formal dining room, expansive living room w/balcony, mini-gourmet kitchen with stainless steel appliances accented by granite counter tops, storage, laundry room, walk-in closet, custom built-in cabinets, hardwood flooring throughout and 1 garage space shared.  Area Amenities:  Centrally located and walking distance to Lake Merritt, shopping, groceries, restaurants, entertainment, public transportation, freeway access and so much more! </span>\n",
      "['# of Baths: 1', 'Master has Shower Over Tub, Master has Tub', '# of Rooms: 4', 'Hardwood Flooring, Linoleum Flooring, Tile Flooring', 'Garage Door Opener', 'Breakfast Bar, Counter (Stone), Dishwasher, Free Standing Range/Oven, Refrigerator, Updated Kitchen', 'In Laundry Room', 'Family Room, Formal Dining Room', 'Steam Heating', 'Has Garage', '# of Garage Spaces: 1', 'Garage Parking', 'High School District: Oakland (510) 879-8111', 'Point of Sale Ordinance', 'Rent Control, Other (Call/See Agent)', 'Water (Public)', '# of Units in Complex: 4', 'Unit Faces Street', 'Existing Construction', 'Stucco Exterior', 'Tar and Gravel Roof', 'Floor # Unit is on: 1', 'Sq. Ft. Source: Public Records', 'TIC % Owner Offered: 25%', 'Parcel #: 023 -0418-031', 'Census Tract: 405200', 'TIC', 'City Transfer Tax', 'Level', 'Back Yard', 'Other', '1 Bedroom, 1 Bath', 'Directions to Property: Lakeshore Ave to Beacon Ave', 'Cross Street: Lakeshore Ave', 'Original Price: $449,950', 'Previous Price: $449,950', \"Possession: Tenant''s Rights, Other\"]\n",
      "['DateEvent & SourcePriceAppreciationJan 18, 2018Sold (MLS) (Sold)Bridge MLS #40787243$390,000—Nov 29, 2017PendingBridge MLS #40787243——Sep 15, 2017Price ChangedBridge MLS #40787243$395,000—See all property history']\n",
      "['Cleveland Elementary SchoolPublic • K to 5 • Serves this homeGreatSchools Rating: 7/10Parent Rating: ', 'Bella Vista Elementary SchoolPublic • K to 5 • Serves this homeGreatSchools Rating: 4/10Parent Rating: ', 'Westlake Middle SchoolPublic • 6 to 8 • Serves this homeGreatSchools Rating: 1/10Parent Rating: ', 'Edna Brewer Middle SchoolPublic • 6 to 8 • Serves this homeGreatSchools Rating: 6/10Parent Rating: ', 'Oakland High SchoolPublic • 9 to 12 • Serves this homeGreatSchools Rating: 4/10Parent Rating: ']\n"
     ]
    }
   ],
   "source": [
    "header = random.sample(ua, 1)[0]\n",
    "proxy = random.sample(proxies, 1)[0]\n",
    "sold_url_list = s_main_df['home_link'].tolist()\n",
    "url = random.sample(sold_url_list, 1)[0]\n",
    "print(proxy)\n",
    "\n",
    "each_property(url, header, proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_regex = r'\\$\\S+\\s+'\n",
    "bed_regex = r'\\d+\\S?\\d?\\d?Bed'\n",
    "bath_regex = r'\\d+\\S?\\d?\\d?Bath'\n",
    "size_regex = r'\\d+\\S?\\d?\\d?\\s?Sq'\n",
    "yr_blt_regex = r'Built: \\d+'\n",
    "status_regex = r'Status: \\w+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
