{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:12.033288Z",
     "start_time": "2018-10-04T20:40:12.016329Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import time, re, ast, sys, urllib, time, random, string, requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from selenium.webdriver.firefox.webdriver import FirefoxProfile\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.error import HTTPError, URLError\n",
    "from itertools import cycle \n",
    "from header_list import user_agent_list\n",
    "from proxies_list import *\n",
    "from INITIAL_SCRAPPER_FUNCTIONS import *\n",
    "from LIST_DF_FUNCTIONS import *\n",
    "from PARSER_FUNCTIONS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:13.092525Z",
     "start_time": "2018-10-04T20:40:13.078169Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ua = user_agent_list\n",
    "proxies = proxies_list_\n",
    "prx_pool = proxie_random_pool\n",
    "# proxie_check(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:13.239603Z",
     "start_time": "2018-10-04T20:40:13.222542Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:13.361630Z",
     "start_time": "2018-10-04T20:40:13.341163Z"
    }
   },
   "outputs": [],
   "source": [
    "# zip_codes = pd.read_csv('../Data/main_zips.csv')\n",
    "# zip_list = zip_codes['zip'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:13.470057Z",
     "start_time": "2018-10-04T20:40:13.454373Z"
    }
   },
   "outputs": [],
   "source": [
    "# active_zip_url_list, sold_zip_url_list = gen_url_list(zip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:13.600422Z",
     "start_time": "2018-10-04T20:40:13.580446Z"
    }
   },
   "outputs": [],
   "source": [
    "# a_main_df = pd.DataFrame(columns=['full_address', 'home_link'])\n",
    "# s_main_df = pd.DataFrame(columns=['full_address', 'home_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:13.708635Z",
     "start_time": "2018-10-04T20:40:13.688904Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = randint(0, 3)\n",
    "# time.sleep(random.uniform(0, 1)*((88/13)**.33)*a)\n",
    "# active_zip_url_list, a_main_df, proxies = links_for_props(proxies, active_zip_url_list, a_main_df, ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:13.822477Z",
     "start_time": "2018-10-04T20:40:13.803862Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = randint(0, 3)\n",
    "# sold_zip_url_list, s_main_df, proxies = links_for_props(proxies, sold_zip_url_list, s_main_df, ua)\n",
    "# time.sleep(random.uniform(0, 1)*((88/13)**.33)*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:13.937948Z",
     "start_time": "2018-10-04T20:40:13.920620Z"
    }
   },
   "outputs": [],
   "source": [
    "# a_main_df.to_csv('../Data/active_props_url_list.csv')\n",
    "# s_main_df.to_csv('../Data/sold_props_url_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:14.100325Z",
     "start_time": "2018-10-04T20:40:14.038871Z"
    }
   },
   "outputs": [],
   "source": [
    "a_main_df = pd.read_csv('../Data/active_props_url_list.csv')\n",
    "s_main_df = pd.read_csv('../Data/sold_props_url_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:14.204550Z",
     "start_time": "2018-10-04T20:40:14.174509Z"
    }
   },
   "outputs": [],
   "source": [
    "a_main_df = a_main_df.drop_duplicates(subset='full_address')\n",
    "s_main_df = s_main_df.drop_duplicates(subset='full_address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:14.910775Z",
     "start_time": "2018-10-04T20:40:14.893124Z"
    }
   },
   "outputs": [],
   "source": [
    "active_url_list = a_main_df.home_link.tolist()\n",
    "sold_url_list = s_main_df.home_link.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:40:46.537896Z",
     "start_time": "2018-10-04T20:40:46.524816Z"
    }
   },
   "outputs": [],
   "source": [
    "def info_from_sold_property(url, hdr, proxy):\n",
    "\n",
    "    soup = session_creator(proxy, ua, url)\n",
    "    top_info_dict = top_info_parser(soup, 1)\n",
    "    public_info_dict = public_info_parser(soup, 1)\n",
    "    school_dict = school_parser(soup, 1)\n",
    "    all_home_feats = feats_parser(soup, 1)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.concat([top_info_dict, school_dict, public_info_dict, \n",
    "                    all_home_feats], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T20:41:55.770355Z",
     "start_time": "2018-10-04T20:41:50.832796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.214.148.134:58427\n",
      "https://www.redfin.com/NC/Kernersville/108-Pineview-Dr-27284/home/94880584\n"
     ]
    }
   ],
   "source": [
    "header = random.sample(ua, 1)[0]\n",
    "proxy = random.sample(proxies, 1)[0]\n",
    "sold_url_list = s_main_df['home_link'].tolist()\n",
    "# url = random.sample(active_url_list, 1)[0]\n",
    "url = random.sample(sold_url_list, 1)[0]\n",
    "print(proxy)\n",
    "print(url)\n",
    "\n",
    "sold_home_df = info_from_sold_property(url, header, proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T22:36:53.276834Z",
     "start_time": "2018-10-04T22:36:53.255999Z"
    }
   },
   "outputs": [],
   "source": [
    "def school_parser(soup):\n",
    "    school_dict = {}\n",
    "    school_info = soup.findAll('div', {'class': \"name-and-info\"})\n",
    "#     print (school_info)\n",
    "    school_names = []\n",
    "    school_grades = []\n",
    "    school_ratings = []\n",
    "    for num in school_info:\n",
    "        s_name = num.findAll('div', {'data-rf-test-name': 'school-name'})\n",
    "        s_grade = num.findAll('div', {'class': re.compile('^sub-info')})\n",
    "        s_rating = num.findAll('div', {'class': 'gs-rating-row'})\n",
    "        for i in s_name:\n",
    "            school_names.append(i.text)\n",
    "        for j in s_grade:\n",
    "            school_grades.append(j.text.replace(' • Serves this home','').replace(' • ',' - '))\n",
    "        for k in s_rating:\n",
    "            school_ratings.append(k.text[-5:].replace(' ','').replace('/10',''))\n",
    "    \n",
    "    w = 0\n",
    "    while w<len(school_names):        \n",
    "        if ('Public' in school_grades[w] and ((\n",
    "            ('k' in school_grades[w] or 'Pre' in school_grades) \n",
    "            or '5' in school_grades[w]) or 'Elementary' in school_names[w])):\n",
    "            school_dict['elem_school_name'] = school_names[w]\n",
    "            school_dict['elem_school_grades'] = school_grades[w].split(' - ',1)[1]\n",
    "            school_dict['elem_school_rating'] = school_ratings[w]\n",
    "            w+=1\n",
    "        else:\n",
    "            w+=1\n",
    "\n",
    "    w = 0\n",
    "    while w<len(school_names):\n",
    "        if ('Public' in school_grades[w] and ((\n",
    "            ('7' in school_grades[w] or '8' in school_grades)\n",
    "            or 'Middle' in school_names[w]) or 'Junior' in school_names[w])):\n",
    "            school_dict['middle_school_name'] = school_names[w].title()\n",
    "            school_dict['middle_school_grades'] = school_grades[w].split(' - ',1)[1].title()\n",
    "            school_dict['middle_school_rating'] = school_ratings[w].title()\n",
    "            w+=1\n",
    "        else:\n",
    "            w+=1\n",
    "\n",
    "    w = 0\n",
    "    while w<len(school_names):\n",
    "        if ('Public' in school_grades[w] and (\n",
    "            ('12' in school_grades or 'High' in school_names[w]))):\n",
    "            school_dict['high_school_name'] = school_names[w].title()\n",
    "            school_dict['high_school_grades'] = school_grades[w].split(' - ',1)[1].title()\n",
    "            school_dict['high_school_rating'] = school_ratings[w].title()\n",
    "            w+=1\n",
    "        else:\n",
    "            w+=1\n",
    "    \n",
    "    if 'elem_school_name' not in school_dict.keys():\n",
    "        school_dict['elem_school_name'] = 'N/A'\n",
    "        school_dict['elem_school_grades'] = 'N/A'\n",
    "        school_dict['elem_school_rating'] = 'N/A'\n",
    "        \n",
    "    if 'middle_school_name' not in school_dict.keys():\n",
    "        school_dict['middle_school_name'] = 'N/A'\n",
    "        school_dict['middle_school_grades'] = 'N/A'\n",
    "        school_dict['middle_school_rating'] = 'N/A'\n",
    "    \n",
    "    if 'high_school_name' not in school_dict.keys():\n",
    "        school_dict['high_school_name'] = 'N/A'\n",
    "        school_dict['high_school_grades'] = 'N/A'\n",
    "        school_dict['high_school_rating'] = 'N/A'\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(school_dict, index=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T22:41:42.104955Z",
     "start_time": "2018-10-04T22:41:28.665626Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.113.168.225:52252\n",
      "https://www.redfin.com/CA/Fremont/1622-Mento-Ter-94539/home/22962572\n",
      "['Joshua Chadbourne Elementary School', 'William Hopkins Junior High School', 'Mission San Jose High School']\n",
      "['Public - K to 6', 'Public - 7 to 8', 'Public - 9 to 12']\n",
      "['8', 'g:NR', '10']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'elem_school_name': 'Joshua Chadbourne Elementary School',\n",
       " 'elem_school_grades': 'K to 6',\n",
       " 'elem_school_rating': '8',\n",
       " 'middle_school_name': 'William Hopkins Junior High School',\n",
       " 'middle_school_grades': '7 To 8',\n",
       " 'middle_school_rating': 'G:Nr',\n",
       " 'high_school_name': 'Mission San Jose High School',\n",
       " 'high_school_grades': '9 To 12',\n",
       " 'high_school_rating': '10'}"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = random.sample(ua, 1)[0]\n",
    "proxy = random.sample(proxies, 1)[0]\n",
    "sold_url_list = s_main_df['home_link'].tolist()\n",
    "url = 'https://www.redfin.com/CA/Fremont/1622-Mento-Ter-94539/home/22962572'\n",
    "# url = random.sample(sold_url_list, 1)[0]\n",
    "print(proxy)\n",
    "print(url)\n",
    "soup = session_creator(proxy, ua, url)\n",
    "school_parser(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
