{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T18:45:21.744700Z",
     "start_time": "2018-09-18T18:45:21.549295Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from selenium.webdriver import Firefox\n",
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import json\n",
    "from random import choice, randint\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.webdriver import FirefoxProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T18:45:22.179105Z",
     "start_time": "2018-09-18T18:45:22.163153Z"
    }
   },
   "outputs": [],
   "source": [
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "reg_property_history_row = re.compile('propertyHistory\\-[0-9]+')\n",
    "reg_property_urls = re.compile('(/[A-Z][A-Z]/[A-Za-z\\-/0-9]+/home/[0-9]+)')\n",
    "user_agent_header = {\n",
    "    'User-agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T18:45:23.285409Z",
     "start_time": "2018-09-18T18:45:23.247667Z"
    }
   },
   "outputs": [],
   "source": [
    "class RedFin():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_url = 'https://www.redfin.com/city/12204/CA/Milpitas/filter/property-type=house+townhouse,min-price=100k,min-baths=1,status=active+pending+contingent'\n",
    "        self.session = requests.Session()\n",
    "        self.use_selenium = False\n",
    "        #  proxy option can be set after class object is loaded\n",
    "        self.use_proxies = False\n",
    "        self.output_data = []\n",
    "        self.property_urls = []\n",
    "        #  load proxies from file one per line proxy:port format\n",
    "        self.proxies = [l.rstrip() for l in open('proxies.txt').readlines()]\n",
    "        #  make a separate session for each proxy\n",
    "        self.sessions = {}\n",
    "        for proxy in self.proxies:\n",
    "            self.sessions[proxy] = {\n",
    "                'session': requests.Session(),\n",
    "                'proxy': {'http': 'http://' + proxy,\n",
    "                          'https': 'https://' + proxy}\n",
    "            }\n",
    "        # load data collected so far in order to avoid needing to scrape\n",
    "        #  the same data twice\n",
    "        try:\n",
    "            self.output_data = json.loads(open('redfin_output.json').read())\n",
    "        except:\n",
    "            self.output_data = []\n",
    "\n",
    "    def rand_sleep(self):\n",
    "        #  you can set the random sleep time for no browser mode here\n",
    "        sleep(randint(5, 10))\n",
    "\n",
    "    def parse_finished_urls(self):\n",
    "        #  function for removing urls that have already completed\n",
    "        done_urls_list = set()\n",
    "        for property_data in self.output_data:\n",
    "            url = property_data['url'][22:]\n",
    "            done_urls_list.add(url)\n",
    "            if url in self.property_urls:\n",
    "                self.property_urls.remove(url)\n",
    "        print(str(len(done_urls_list)) + ' properties already done')\n",
    "        print(str(len(self.property_urls)) + ' proeprties to go')\n",
    "\n",
    "    def get_search_results(self):\n",
    "        page_source = self.request_search_page(self.start_url)\n",
    "        self.property_urls = reg_property_urls.findall(\n",
    "            page_source.replace('\\\\u002F', '/'))\n",
    "        self.property_urls = list(set(self.property_urls))\n",
    "        print('found ' + str(len(self.property_urls)) + ' results')\n",
    "        self.parse_finished_urls()\n",
    "\n",
    "    def request_search_page(self, page_url):\n",
    "        if self.use_selenium:\n",
    "            return self.get_page_selenium(page_url)\n",
    "        else:\n",
    "            return self.make_page_request(page_url)\n",
    "\n",
    "    def get_property_data(self):\n",
    "        count = 0\n",
    "        for property_url in self.property_urls:\n",
    "            self.output_data.append(self.get_property_page(property_url))\n",
    "            count += 1\n",
    "            print('finished page ' + str(count))\n",
    "            open('redfin_output.json', 'w').write(\n",
    "                json.dumps(self.output_data, indent=4))\n",
    "\n",
    "    def make_page_request(self, property_url):\n",
    "        self.rand_sleep()\n",
    "        if self.use_selenium:\n",
    "            return self.get_page_selenium('https://www.redfin.com' + property_url)\n",
    "        elif self.use_proxies:\n",
    "            return self.make_page_request_proxy(property_url)\n",
    "        else:\n",
    "            return self.make_page_request_no_proxy(property_url)\n",
    "\n",
    "    def make_page_request_no_proxy(self, property_url):\n",
    "        #  use a loop to handle various http request errors and retry\n",
    "        #  if 10 fails reached assume we've been blcoked\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                http_response = self.session.get(\n",
    "                    property_url, headers=user_agent_header, verify=False)\n",
    "                if http_response.status_code == 200:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(1, 'Request error')\n",
    "            if i == 9:\n",
    "                print(1, 'blocked error')\n",
    "                exit()\n",
    "        return http_response.text\n",
    "\n",
    "    def make_page_request_proxy(self, property_url):\n",
    "        #  use a loop to handle various http request errors and retry\n",
    "        #  if 10 fails reached assume we've been blcoked\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                session = self.sessions[choice(self.proxies)]\n",
    "                http_response = session['session'].get(property_url, headers=user_agent_header,\n",
    "                                                       proxies=session['proxy'], verify=False)\n",
    "                if http_response.status_code == 200:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(2, 'Request error')\n",
    "            if i == 9:\n",
    "                print(2, 'blocked error')\n",
    "                exit()\n",
    "        return http_response.text\n",
    "\n",
    "    def get_property_page(self, property_url):\n",
    "        page_source = self.make_page_request(property_url)\n",
    "        return self.parse_property_page(page_source, property_url)\n",
    "\n",
    "    def parse_property_page(self, page_source, property_url):\n",
    "        self.soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        property_data = OrderedDict()\n",
    "\n",
    "        #  use try catch to handle when a data point is not available\n",
    "        try:\n",
    "            property_data['street_address'] = self.soup.find(\n",
    "                'span', attrs={'itemprop': 'streetAddress'}).get_text()\n",
    "        except:\n",
    "            property_data['street_address'] = 'N/A'\n",
    "            print('street_address not found')\n",
    "        try:\n",
    "            property_data['address_locality'] = self.soup.find(\n",
    "                'span', attrs={'itemprop': 'addressLocality'}).get_text()\n",
    "        except:\n",
    "            property_data['address_locality'] = 'N/A'\n",
    "            print('address_locality not found')\n",
    "        try:\n",
    "            property_data['address_region'] = self.soup.find(\n",
    "                'span', attrs={'itemprop': 'addressRegion'}).get_text()\n",
    "        except:\n",
    "            property_data['address_region'] = 'N/A'\n",
    "            print('address_region not found')\n",
    "        try:\n",
    "            property_data['postal_code'] = self.soup.find(\n",
    "                'span', attrs={'itemprop': 'postalCode'}).get_text()\n",
    "        except:\n",
    "            property_data['postal_code'] = 'N/A'\n",
    "            print('postal_code not found')\n",
    "        try:\n",
    "            property_data['price'] = self.soup.find(\n",
    "                'div', attrs={'class': 'info-block price'}).find('div').get_text()\n",
    "        except:\n",
    "            property_data['price'] = 'N/A'\n",
    "            print('price not found')\n",
    "        try:\n",
    "            property_data['beds'] = self.soup.find(\n",
    "                'div', attrs={'data-rf-test-id': 'abp-beds'}).find('div').get_text()\n",
    "        except:\n",
    "            property_data['beds'] = 'N/A'\n",
    "            print('beds not found')\n",
    "        try:\n",
    "            property_data['baths'] = self.soup.find('div', attrs={'data-rf-test-id': 'abp-baths'}).find(\n",
    "                'div').get_text()\n",
    "        except:\n",
    "            property_data['baths'] = 'N/A'\n",
    "            print('baths not found')\n",
    "        try:\n",
    "            property_data['sqFt'] = self.soup.find('div', attrs={'data-rf-test-id': 'abp-sqFt'}).find('span', attrs={\n",
    "                'class': 'main-font statsValue'}).get_text()\n",
    "        except:\n",
    "            property_data['sqFt'] = 'N/A'\n",
    "            print('sqFt not found')\n",
    "        try:\n",
    "            property_data['price_per_sqFt'] = self.soup.find('div', attrs={'data-rf-test-id': 'abp-sqFt'}).find('div',\n",
    "                                                                                                                attrs={\n",
    "                                                                                                                    \"data-rf-test-id\": \"abp-priceperft\"}).get_text()\n",
    "        except:\n",
    "            property_data['price_per_sqFt'] = 'N/A'\n",
    "            print('price_per_sqFt not found')\n",
    "        try:\n",
    "            property_data['year_built'] = self.soup.find('span', attrs={\"data-rf-test-id\": \"abp-yearBuilt\"}).find(\n",
    "                'span', attrs={'class': 'value'}).get_text()\n",
    "        except:\n",
    "            property_data['year_built'] = 'N/A'\n",
    "            print('year_built not found')\n",
    "        try:\n",
    "            property_data['days_on_redfin'] = self.soup.find('span',\n",
    "                                                             attrs={\"data-rf-test-id\": \"abp-daysOnRedfin\"}).find('span',\n",
    "                                                                                                                 attrs={\n",
    "                                                                                                                     'class': 'value'}).get_text()\n",
    "        except:\n",
    "            property_data['days_on_redfin'] = 'N/A'\n",
    "            print('days_on_redfin not found')\n",
    "        try:\n",
    "            property_data['status'] = self.soup.find('span', attrs={\"data-rf-test-id\": \"abp-status\"}).find('span',\n",
    "                                                                                                           attrs={\n",
    "                                                                                                               'class': 'value'}).get_text()\n",
    "        except:\n",
    "            property_data['status'] = 'N/A'\n",
    "            print('status not found')\n",
    "\n",
    "        property_data['summary'] = self.soup.find(\n",
    "            'div', attrs={'class': 'remarks'}).get_text()\n",
    "        for row in self.soup.find('div', attrs={'class': 'more-info-div'}).find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            property_data[cells[0].get_text().strip()\n",
    "                          ] = cells[1].get_text().strip()\n",
    "\n",
    "        # use loops to maintain data structure ina dict\n",
    "        property_data['property_details'] = OrderedDict()\n",
    "        for category in self.soup.find('div', attrs={'class': 'amenities-container'}).children:\n",
    "            key = category.contents[0].get_text().strip()\n",
    "            property_data['property_details'][key] = OrderedDict()\n",
    "            for row in category.contents[1].find_all('div', attrs={'class': 'amenity-group'}):\n",
    "                key2 = row.find('h4').get_text()\n",
    "                property_data['property_details'][key][key2] = []\n",
    "                for row2 in row.find_all('li'):\n",
    "                    property_data['property_details'][key][key2].append(\n",
    "                        row2.get_text())\n",
    "\n",
    "        property_data['propert_history'] = []\n",
    "        for row in self.soup.find_all('tr', attrs={'id': reg_property_history_row}):\n",
    "            data_cells = row.find_all('td')\n",
    "            history_data_row = OrderedDict()\n",
    "            history_data_row['date'] = data_cells[0].get_text()\n",
    "            history_data_row['event & source'] = data_cells[1].get_text()\n",
    "            history_data_row['price'] = data_cells[2].get_text()\n",
    "            history_data_row['appreciation'] = data_cells[3].get_text()\n",
    "            property_data['propert_history'].append(history_data_row)\n",
    "\n",
    "        property_data['url'] = 'https://www.redfin.com' + property_url\n",
    "        self.output_data.append(property_data)\n",
    "        return property_data\n",
    "\n",
    "    def use_browser(self):\n",
    "        self.use_selenium = True\n",
    "        firefox_profile = FirefoxProfile()\n",
    "        #  might as well turn off images since we don't need them\n",
    "        if self.use_proxies:\n",
    "            #  if use proxies is true load firefox with proxies\n",
    "            firefox_profile.set_preference(\"permissions.default.image\", 2)\n",
    "            proxy_host, proxy_port = choice(self.proxies).split(':')\n",
    "            firefox_profile.set_preference(\"network.proxy.type\", 1)\n",
    "            firefox_profile.set_preference(\"network.proxy.http\", proxy_host)\n",
    "            firefox_profile.set_preference(\n",
    "                \"network.proxy.http_port\", int(proxy_port))\n",
    "            firefox_profile.set_preference(\"network.proxy.ssl\", proxy_host)\n",
    "            firefox_profile.set_preference(\n",
    "                \"network.proxy.ssl_port\", int(proxy_port))\n",
    "        self.driver = Firefox(firefox_profile)\n",
    "        self.driver.implicitly_wait(2)\n",
    "\n",
    "    def get_page_selenium(self, page_url):\n",
    "        self.driver.get(page_url)\n",
    "        self.selenium_bypass_captcha()\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def selenium_bypass_captcha(self):\n",
    "        #  basic code for handling captcha\n",
    "        #  this requires the user to actually solve the captcha and then continue\n",
    "        try:\n",
    "            self.driver.switch_to_frame(self.driver.find_element_by_xpath(\n",
    "                '//iframe[@title=\"recaptcha widget\"]'))\n",
    "            self.driver.find_element_by_class_name(\n",
    "                'recaptcha-checkbox-checkmark').click()\n",
    "            print('solve captcha ( pop up only ) and press enter to continue')\n",
    "            raw_input()\n",
    "            self.driver.switch_to_default_content()\n",
    "            self.driver.find_element_by_id('submit').click()\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T18:45:32.274911Z",
     "start_time": "2018-09-18T18:45:24.182220Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RedFin' object has no attribute 'make_page_request_no_proxy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b93b23d1adf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mredfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRedFin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mredfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_search_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mredfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_property_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-36ee956f6434>\u001b[0m in \u001b[0;36mget_search_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_search_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mpage_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_search_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         self.property_urls = reg_property_urls.findall(\n\u001b[1;32m     46\u001b[0m             page_source.replace('\\\\u002F', '/'))\n",
      "\u001b[0;32m<ipython-input-3-36ee956f6434>\u001b[0m in \u001b[0;36mrequest_search_page\u001b[0;34m(self, page_url)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_page_selenium\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_page_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_property_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-36ee956f6434>\u001b[0m in \u001b[0;36mmake_page_request\u001b[0;34m(self, property_url)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_page_request_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperty_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_page_request_no_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperty_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m#     def make_page_request_no_proxy(self, property_url):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RedFin' object has no attribute 'make_page_request_no_proxy'"
     ]
    }
   ],
   "source": [
    "redfin = RedFin()\n",
    "redfin.get_search_results()\n",
    "redfin.get_property_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
